{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60e02ec0-6443-477e-9a4e-6167eaaabf8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tourloid/Desktop/PhD/Code/SPVD\n"
     ]
    }
   ],
   "source": [
    "# cd to the main directory\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d609e6b-b8e8-46af-8b46-c73bf3cf1f49",
   "metadata": {},
   "source": [
    "# Create a base dataset to load raw data from PartNet\n",
    "\n",
    "This initial version of PartNet is used to load the raw data in order to process them. This is not used as a training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec80fdcc-8a28-45af-9994-2f34c6637fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import h5py\n",
    "import glob\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import json\n",
    "from datasets.utils import NoiseSchedulerDDPM\n",
    "import random\n",
    "from torchsparse.utils.quantize import sparse_quantize\n",
    "from torchsparse import SparseTensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torchsparse.utils.collate import sparse_collate_fn\n",
    "\n",
    "categories = ['Faucet', 'Chair', 'Display', 'Knife', 'Table', 'Laptop', 'Refrigerator', 'Microwave', \n",
    "              'StorageFurniture', 'Bowl', 'Scissors', 'Door', 'TrashCan','Bed', 'Keyboard', 'Clock', \n",
    "              'Bottle', 'Bag', 'Lamp', 'Earphone', 'Vase', 'Dishwasher', 'Mug', 'Hat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf2d676f-de7a-4734-ae9c-b7ffb4804d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "class PartNet(Dataset):\n",
    "    \"\"\"\n",
    "        This dataset class loads data from .h5 and .json files and returns them unprocessed. \n",
    "    \"\"\"\n",
    "    def __init__(self, root_path, category, split='train', n_points=10000):\n",
    "        super().__init__()\n",
    "\n",
    "        all_categories = [f for f in os.listdir(root_path) if os.path.isdir(os.path.join(root_path, f))]\n",
    "        assert category in all_categories, all_categories\n",
    "        assert split in ['train', 'test', 'val'], split\n",
    "\n",
    "        self.category = category\n",
    "        self.split = split\n",
    "        # create the path for the specific category\n",
    "        category_path = os.path.join(root_path, category)\n",
    "        \n",
    "        # read all .h5 files and .json files\n",
    "        self.points, self.labels, self.colors, self.meta = self.load_files(category_path, split)\n",
    "        self.points = self.points[:, :n_points]\n",
    "        self.labels = self.labels[:, :n_points]\n",
    "        self.colors = self.colors[:, :n_points]\n",
    "\n",
    "    def load_files(self, category_path, split):\n",
    "        all_points, all_labels, all_colors = [], [], []\n",
    "        all_meta = []\n",
    "        file_pattern = f'{split}*.h5'\n",
    "        file_paths = glob.glob(os.path.join(category_path, file_pattern))\n",
    "        \n",
    "        for file_path in file_paths:\n",
    "            with h5py.File(file_path, 'r') as f:\n",
    "                points = f['pts'][:]\n",
    "                labels = f['label'][:]\n",
    "                colors = f['rgb'][:]\n",
    "                all_points.append(points)\n",
    "                all_labels.append(labels)\n",
    "                all_colors.append(colors)\n",
    "\n",
    "            json_file = file_path.split('.')[0] + '.json'\n",
    "            with open(json_file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                all_meta.extend(data)\n",
    "\n",
    "        all_points = np.concatenate(all_points, axis=0)\n",
    "        all_labels = np.concatenate(all_labels, axis=0)\n",
    "        all_colors = np.concatenate(all_colors, axis=0)\n",
    "        \n",
    "        return all_points, all_labels, all_colors, all_meta\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.points)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        pc, label, color, metadata = self.points[idx], self.labels[idx], self.colors[idx], self.meta[idx]\n",
    "        return pc, label, color, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0e318fd-bbc4-4c77-a2a1-4a5a9a44f21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/tourloid/Downloads/PartNet/ins_seg_h5/ins_seg_h5/'\n",
    "partnet = PartNet(path, 'Chair', 'train', n_points=2048)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1340ba23-59cc-4806-b6bf-4bb56c7149b0",
   "metadata": {},
   "source": [
    "# Process the dataset\n",
    "\n",
    "We aim to create a version of the dataset where the subparts of objects have a substantial number of points, thereby excluding the very small parts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23036dc5-d9a6-4ff2-a1da-cc64294f0e01",
   "metadata": {},
   "source": [
    "## Represent the data as a tree structure\n",
    "Note: This code needs refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "37906216-ab5b-4ad3-b6dd-40fdb484b4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeNode:\n",
    "    def __init__(self, name, parent=None):\n",
    "        self.name = name\n",
    "        self.parent = parent\n",
    "        self.categories = set()\n",
    "        self.children = {}\n",
    "        self.label_count = 0  # To store frequency of the label in the point cloud\n",
    "\n",
    "    def add_child(self, child, category=None, is_leaf=False):\n",
    "        identifier = f\"{child}_{len(self.children)}\" if is_leaf else child\n",
    "        if identifier not in self.children:\n",
    "            new_node = TreeNode(child, self)\n",
    "            if category is not None:\n",
    "                if isinstance(category, list):  # Handle list of categories\n",
    "                    new_node.categories.update(category)\n",
    "                else:\n",
    "                    new_node.categories.add(category)\n",
    "            self.children[identifier] = new_node\n",
    "        return self.children[identifier]\n",
    "\n",
    "    def __repr__(self, level=0):\n",
    "        category_str = ', '.join(map(str, self.categories)) if self.categories else 'None'\n",
    "        ret = \"\\t\" * level + repr(self.name) + f\" (Categories: {category_str}, Count: {self.label_count})\\n\"\n",
    "        for child in self.children.values():\n",
    "            ret += child.__repr__(level + 1)\n",
    "        return ret\n",
    "\n",
    "\n",
    "class Tree:\n",
    "    def __init__(self):\n",
    "        self.root = TreeNode('root')\n",
    "\n",
    "    def insert(self, path, category):\n",
    "        parts = path.split('/')\n",
    "        current_node = self.root\n",
    "        for i, part in enumerate(parts):\n",
    "            is_leaf = (i == len(parts) - 1)\n",
    "            current_node = current_node.add_child(part, category if is_leaf else None, is_leaf)\n",
    "\n",
    "    def update_node_label_count(self, category, count):\n",
    "        def update(node):\n",
    "            if category in node.categories:\n",
    "                node.label_count += count\n",
    "            for child in node.children.values():\n",
    "                update(child)\n",
    "        update(self.root)\n",
    "\n",
    "    def merge_low_count_leaves(self, threshold):\n",
    "        def collect_leaves(node, leaves):\n",
    "            if node.children:\n",
    "                for child in node.children.values():\n",
    "                    collect_leaves(child, leaves)\n",
    "            else:\n",
    "                leaves.append(node)\n",
    "\n",
    "        leaves = []\n",
    "        collect_leaves(self.root, leaves)\n",
    "\n",
    "        # Filter leaves below the threshold\n",
    "        low_count_leaves = [leaf for leaf in leaves if leaf.label_count < threshold]\n",
    "        name_to_leaves = {}\n",
    "        for leaf in low_count_leaves:\n",
    "            name_to_leaves.setdefault(leaf.name.split('_')[0], []).append(leaf)\n",
    "\n",
    "        # Merge leaves with the same base name\n",
    "        for name, leaves in name_to_leaves.items():\n",
    "            if len(leaves) > 1:\n",
    "                # Accumulate counts and categories\n",
    "                target_leaf = leaves[0]\n",
    "                for leaf in leaves[1:]:\n",
    "                    target_leaf.label_count += leaf.label_count\n",
    "                    target_leaf.categories.update(leaf.categories)\n",
    "                    # Remove leaf from parent\n",
    "                    if leaf.parent:\n",
    "                        keys_to_remove = [key for key, val in leaf.parent.children.items() if val == leaf]\n",
    "                        for key in keys_to_remove:\n",
    "                            del leaf.parent.children[key]\n",
    "\n",
    "    def get_leaf_nodes(self):\n",
    "        leaves = []\n",
    "        def collect_leaves(node):\n",
    "            if not node.children:  # If no children, it's a leaf node\n",
    "                leaves.append(node)\n",
    "            else:\n",
    "                for child in node.children.values():\n",
    "                    collect_leaves(child)\n",
    "        collect_leaves(self.root)\n",
    "        return leaves\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.root.__repr__()\n",
    "\n",
    "    def display_tree(self):\n",
    "        print(self.__repr__())\n",
    "\n",
    "def build_tree(node_info):\n",
    "    tree = Tree()\n",
    "    for path, category in node_info:\n",
    "        tree.insert(path, category)\n",
    "    return tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "32cb38cc-26dd-4d24-ba3e-7c40b5c81fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(partnet, threshold=300):\n",
    "    all_pc = []\n",
    "    all_labels = []\n",
    "    all_colors = []\n",
    "    \n",
    "    \n",
    "    for data in partnet:\n",
    "    \n",
    "        pc, labels, colors, metadata = data\n",
    "        part_meta = metadata['ins_seg']\n",
    "    \n",
    "        part_names = []\n",
    "        part_ids = []\n",
    "    \n",
    "        for item in part_meta:\n",
    "            part_name = item['part_name']\n",
    "            part_names.append(part_name)\n",
    "    \n",
    "            leaf_id = item['leaf_id_list']\n",
    "            part_ids.append(leaf_id)\n",
    "    \n",
    "        node_info = [(node_name, part_id) for node_name, part_id in zip(part_names, part_ids)]\n",
    "    \n",
    "        filtered_node_info = []\n",
    "        for ni in node_info:\n",
    "            if len(ni[1]) > 1: continue\n",
    "            filtered_node_info.append(ni)\n",
    "        \n",
    "        # use the filtered node info to create a tree structure of the point cloud\n",
    "        tree = build_tree(filtered_node_info)\n",
    "        \n",
    "        # update the number of points in each label\n",
    "        unique, counts = np.unique(labels, return_counts=True)\n",
    "        label_frequencies = dict(zip(unique, counts))\n",
    "    \n",
    "        for label, count in label_frequencies.items():\n",
    "            tree.update_node_label_count(label, count)\n",
    "\n",
    "\n",
    "        # merge nodes with a low number of points\n",
    "        tree.merge_low_count_leaves(threshold)\n",
    "\n",
    "        leaf_nodes = tree.get_leaf_nodes()\n",
    "        \n",
    "        for node in leaf_nodes: \n",
    "            cats = node.categories\n",
    "            if len(cats) > 1:\n",
    "                main_cat = cats.pop()\n",
    "                for c in cats:\n",
    "                    inds = labels == c\n",
    "                    labels[inds] = main_cat\n",
    "\n",
    "        num_parts = len(np.unique(labels))\n",
    "        \n",
    "        all_pc.append(pc)\n",
    "        all_labels.append(labels)\n",
    "        all_colors.append(colors)\n",
    "    \n",
    "    all_pc = np.stack(all_pc)\n",
    "    all_labels = np.stack(all_labels)\n",
    "    all_colors = np.stack(all_colors)\n",
    "\n",
    "    return all_pc, all_labels, all_colors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f0bd4d-eda6-4573-be79-d80873de9e1b",
   "metadata": {},
   "source": [
    "## Creation of the dataset - Preprocessing the data\n",
    "To generate the data run the following function: See example bellow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e13202c3-13a5-4a29-a922-a5540e0c2cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(data_path, categories, save_path='./data/PartNetProcessed/', n_points=2048, threshold=40):\n",
    "    splits = ['train', 'test', 'val']\n",
    "    for category in categories:\n",
    "        for split in splits:\n",
    "            dataset = PartNet(path, category, split, n_points)\n",
    "            all_pc, all_labels, _ = process_dataset(dataset, threshold=threshold)\n",
    "            np.save(os.path.join(save_path, f'{category}_{split}_points.npy'), all_pc)\n",
    "            np.save(os.path.join(save_path, f'{category}_{split}_labels.npy'), all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fe6f33bb-0d8c-42f1-a8f8-ebc7fe713eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_data(path, categories = ['Chair', 'Table'], threshold=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981cc183-0cfe-4aee-b4f8-dbbfa53cc4f0",
   "metadata": {},
   "source": [
    "## Create a Dataset class to load the generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29c67633-1747-44cf-8d44-2346a084e9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class PartNetCompletion(Dataset):\n",
    "\n",
    "    def __init__(self, root_path, category, split='train', n_points=2048):\n",
    "        super().__init__()\n",
    "\n",
    "        assert split in ['train', 'test', 'val'], split\n",
    "\n",
    "        # path to load points and labels\n",
    "        point_path = os.path.join(root_path, f'{category}_{split}_points.npy')\n",
    "        label_path = os.path.join(root_path, f'{category}_{split}_labels.npy')\n",
    "\n",
    "        # load points and labels\n",
    "        points = np.load(point_path)\n",
    "        labels = np.load(label_path)\n",
    "\n",
    "        # keep n_points\n",
    "        self.points = points[:, :n_points]\n",
    "        self.labels = labels[:, :n_points]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.points)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pc, labels = self.points[idx], self.labels[idx]  \n",
    "        return pc, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63daadd6-c331-40ec-9c1c-aca42009c8e0",
   "metadata": {},
   "source": [
    "### Visualize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74616a5a-985e-46fe-a4b2-5b44a264ffec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "from utils.visualization import vis_pc_sphere\n",
    "\n",
    "def generate_colors(num_labels):\n",
    "    # Generates a unique color for each label\n",
    "    return np.random.rand(num_labels, 3)  # Random RGB colors\n",
    "\n",
    "def label_to_color_tensor(point_labels):\n",
    "    # Flatten to handle in a single array (assumes point_labels is a 1D array of labels for simplicity)\n",
    "    unique_labels = np.unique(point_labels)\n",
    "    num_labels = len(unique_labels)\n",
    "    \n",
    "    # Generate colors\n",
    "    colors = generate_colors(num_labels)\n",
    "    \n",
    "    # Create a dictionary mapping label to color\n",
    "    label_to_color_map = {label: colors[i] for i, label in enumerate(unique_labels)}\n",
    "    \n",
    "    # Map each point's label to its color\n",
    "    color_tensor = np.array([label_to_color_map[label] for label in point_labels])\n",
    "    \n",
    "    return color_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1b03c5b-2372-4661-ba08-56b937722b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PartNetCompletion('./data/PartNetProcessed/', 'Chair')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "812945f5-dc86-41d7-a9a9-721ecf3d0bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "points, labels = dataset[0]\n",
    "color = label_to_color_tensor(labels)\n",
    "vis_pc_sphere(torch.tensor(points), radius=0.05, color=torch.tensor(color))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4b94cf-9ac5-4f3c-9129-fc8b65775ce1",
   "metadata": {},
   "source": [
    "### Dataset for Completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2c1f93ff-7cee-462e-a2f1-d365f8829ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "class PartNetCompletionNoisySparse(PartNetCompletion):\n",
    "\n",
    "    def __init__(self, root_path, category, split='train', n_points=2048, pres=1e-5, min_num_parts=3, add_real_info=True,\n",
    "                  ddpm_params = {'beta_min':0.0001, \n",
    "                                 'beta_max':0.02, \n",
    "                                 'n_steps' :1000, \n",
    "                                 'mode'    :'linear'}):\n",
    "        super().__init__(root_path, category, split, n_points)\n",
    "        self.pres = pres\n",
    "        self.noise_scheduler = NoiseSchedulerDDPM(beta_min=ddpm_params['beta_min'],\n",
    "                                                  beta_max=ddpm_params['beta_max'],\n",
    "                                                  n_steps =ddpm_params['n_steps'],\n",
    "                                                  mode    =ddpm_params['mode'])\n",
    "\n",
    "        self.min_num_parts = min_num_parts\n",
    "        self.add_real_info = add_real_info\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        points, labels = super().__getitem__(idx)\n",
    "\n",
    "        # normalize point cloud (mean=0, std=1)\n",
    "        points = points - points.mean(axis=0)\n",
    "        points = points / points.std()\n",
    "        \n",
    "        # select parts to discard\n",
    "        unique_labels = np.unique(labels)\n",
    "        num_parts = len(unique_labels)\n",
    "        if num_parts > self.min_num_parts:\n",
    "            keep_parts = random.randint(self.min_num_parts, num_parts - 1)\n",
    "        else:\n",
    "            keep_parts = random.randint(1, num_parts-1)\n",
    "            \n",
    "        keep_labels = np.random.choice(unique_labels, keep_parts, replace=False)\n",
    "        \n",
    "        # create a mask        \n",
    "        labels = torch.tensor(labels)\n",
    "        keep_labels = torch.tensor(keep_labels)\n",
    "        mask = torch.isin(labels, keep_labels)\n",
    "\n",
    "        # add noise\n",
    "        points = torch.tensor(points)\n",
    "        noisy_pc, t, noise = self.noise_scheduler(points)\n",
    "\n",
    "        # keep the original points for the masked areas\n",
    "        noisy_pc[mask] = points[mask]\n",
    "                \n",
    "        # voxelize\n",
    "        coords = noisy_pc.numpy()\n",
    "        coords = coords - np.min(coords, axis=0, keepdims=True)\n",
    "        coords, indices = sparse_quantize(coords, self.pres, return_index=True)\n",
    "\n",
    "        coords = torch.tensor(coords)\n",
    "\n",
    "        if self.add_real_info:\n",
    "            noisy_pc = torch.cat([noisy_pc, mask.unsqueeze(-1).float()], dim=-1)\n",
    "        \n",
    "        feats = noisy_pc[indices]\n",
    "        noise = noise[indices]\n",
    "        mask  = mask[indices]\n",
    "        \n",
    "        noisy_pc = SparseTensor(coords=coords, feats=feats)\n",
    "        \n",
    "        return {\n",
    "            'input':noisy_pc,\n",
    "            't': t,\n",
    "            'noise': noise,\n",
    "            'mask': mask\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5e85bcec-27f3-4763-996c-37ed84c835cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PartNetCompletionNoisySparse('./data/PartNetProcessed/', 'Chair')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cb24b574-f41c-4f09-8db3-7806cea67e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = dataset[0]\n",
    "pc, mask = res['input'], res['mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0dc6f334-85b6-4742-86c8-d335eb91a9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_pc_sphere(pc.F[mask, :3], radius=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "40caf592-02a8-4f01-89a3-1a9915d78f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "def get_sparse_completion_datasets(path, category, n_points=2048, pres=1e-5, min_num_parts=3, add_real_info=True,\n",
    "                                    ddpm_params = {'beta_min':0.0001, \n",
    "                                                   'beta_max':0.02, \n",
    "                                                   'n_steps':1000, \n",
    "                                                   'mode':'linear' }):\n",
    "\n",
    "    tr_dataset = PartNetCompletionNoisySparse(path, category, split='train', n_points=n_points, pres=pres, min_num_parts=min_num_parts, \n",
    "                                                  add_real_info=add_real_info, ddpm_params=ddpm_params) \n",
    "    te_dataset = PartNetCompletionNoisySparse(path, category, split='test', n_points=n_points, pres=pres, min_num_parts=min_num_parts, \n",
    "                                                  add_real_info=add_real_info, ddpm_params=ddpm_params)\n",
    "    \n",
    "    return tr_dataset, te_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "51d49237-8ab9-45d8-9099-656a088ae734",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "def get_sparse_completion_dataloaders(path, category, n_points=2048, pres=1e-5, min_num_parts=3, add_real_info=True, batch_size=32, num_workers=8,\n",
    "                                       ddpm_params = {'beta_min':0.0001, \n",
    "                                                      'beta_max':0.02, \n",
    "                                                      'n_steps':1000, \n",
    "                                                      'mode':'linear'}):\n",
    "    tr_dataset, te_dataset = get_sparse_completion_datasets(path, category=category, n_points=n_points, pres=pres, min_num_parts=min_num_parts, \n",
    "                                                            add_real_info=add_real_info, ddpm_params=ddpm_params)\n",
    "\n",
    "    tr_dl = DataLoader(tr_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, drop_last=True, collate_fn=sparse_collate_fn)\n",
    "    te_dl = DataLoader(te_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, collate_fn=sparse_collate_fn)\n",
    "\n",
    "    return tr_dl, te_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e65c02b8-c929-466a-a5b8-b17132326846",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(140, 39)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_dl, te_dl = get_sparse_completion_dataloaders('./data/PartNetProcessed/', 'Chair')\n",
    "len(tr_dl), len(te_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "57de2ecd-e289-44b9-a78a-bd9125c33cf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4489, 1217)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tr_dl.dataset), len(te_dl.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1ec695db-a925-4d6c-b3c7-bf6c10a220b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in tr_dl:\n",
    "    pc = batch['input']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb12c47-f835-417d-b8fa-75733dc7b41e",
   "metadata": {},
   "source": [
    "# Generation of Colored Point Cloud Data\n",
    "- Use farthest point sample to sample the points -- later note: fps is not required..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e928cade-bfc4-47fe-82ca-e4213c63a184",
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917031e2-b091-4b16-a2b7-ed646644ce6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/tourloid/Downloads/PartNet/ins_seg_h5/ins_seg_h5/'\n",
    "categories = ['Chair']\n",
    "splits = ['train', 'test', 'val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d510b09-89fb-4203-9753-cf1e4bb1ba4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(dataset, save_path = './data/ShapeNetColor', s_points=10000):\n",
    "    category, split = dataset.category, dataset.split\n",
    "    print(f' Processing PartNet category : {category} | split: {split}')\n",
    "    all_pcs = []\n",
    "    all_clr = []\n",
    "    for data in dataset:\n",
    "        pc, label, color, metadata = data\n",
    "        color = color.astype(np.float32) / 255.\n",
    "        \n",
    "        # create open3d point cloud\n",
    "        pcd = o3d.geometry.PointCloud()\n",
    "        pcd.points = o3d.utility.Vector3dVector(pc)\n",
    "        pcd.colors = o3d.utility.Vector3dVector(color)\n",
    "        \n",
    "        # fps \n",
    "        pcd_fps = pcd.farthest_point_down_sample(s_points)\n",
    "        \n",
    "        fps_points = np.asarray(pcd_fps.points)\n",
    "        fps_colors = np.asarray(pcd_fps.colors)\n",
    "    \n",
    "        all_pcs.append(fps_points)\n",
    "        all_clr.append(fps_colors)\n",
    "    \n",
    "    all_pcs = np.stack(all_pcs)\n",
    "    all_clr = np.stack(all_clr)\n",
    "\n",
    "    all_clr_pcs = np.concatenate([all_pcs, all_clr], axis=-1)\n",
    "\n",
    "    name = f'{category}_{split}.npy'\n",
    "    np.save(os.path.join(save_path, name), all_clr_pcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97aff4b1-b8bc-429c-ab5b-b9c1b8e18270",
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in categories:\n",
    "    for split in splits:\n",
    "        partnet = PartNet(path, category, split)\n",
    "        process_dataset(partnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4b6c6a-0019-4aa7-9af0-4b3c8c02567e",
   "metadata": {},
   "source": [
    "## Create a dataset that will load these processed pointclouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e75405-4e20-48ad-aad3-dcaf8966548c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "class ShapeNetColor(Dataset):\n",
    "    def __init__(self, root_path, category, split='train', n_points=2048, max_points=10000):\n",
    "        '''\n",
    "        Processing: \n",
    "            1. Normalize color values globally (dataset provides inverse function to retrieve correct color values)\n",
    "            2. Normalize point coordinates per shape ==> mean=0, std=1\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "        self.category, self.split, self.n_points, self.max_points = category, split, n_points, max_points\n",
    "        \n",
    "        data_path = os.path.join(root_path, f'{category}_{split}.npy')\n",
    "        data = np.load(data_path)\n",
    "        data = data[:, :max_points, :]\n",
    "\n",
    "        # normalize color values\n",
    "        self.m = data[..., 3:].mean(axis=1).mean(axis=0)\n",
    "        data[..., 3:] = data[..., 3:] - self.m\n",
    "        self.s = data[..., 3:].std()\n",
    "        data[..., 3:] = data[..., 3:] / self.s\n",
    "\n",
    "        self.data = data\n",
    "\n",
    "    def get_actual_color(self, clr):\n",
    "        return (clr * self.s + self.m).clamp(0, 1)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data[idx]\n",
    "\n",
    "        if self.max_points > self.n_points:\n",
    "            # select a random sabsample of the points\n",
    "            indices = np.arange(self.n_points)\n",
    "            np.random.shuffle(indices)\n",
    "            data = data[indices]\n",
    "\n",
    "        # normalize coordinates\n",
    "        data[:, :3] = data[:, :3] - data[:, :3].mean(axis=0)\n",
    "        data[:, :3] = data[:, :3] / data[:, :3].std()\n",
    "\n",
    "        return torch.tensor(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5f73b6-3639-4fc4-9906-4cad7d8d3cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "shapenetcolour = ShapeNetColor('./data/ShapeNetColor/', category='Chair', max_points=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff75282f-2063-4522-a76f-75a9841a08d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "shapenetcolour[0].mean(axis=0), shapenetcolour[0].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6249908-ee4e-46fd-84ab-82154b3e88f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "class ShapeNetColorSparseNoisy(ShapeNetColor):\n",
    "\n",
    "    def __init__(self, root_path, category, split='train', n_points=2048, max_points=10000, \n",
    "                 pres=1e-5, ddpm_params = {'beta_min':0.0001, \n",
    "                                           'beta_max':0.02, \n",
    "                                           'n_steps':1000, \n",
    "                                           'mode':'linear' }):\n",
    "        super().__init__(root_path, category, split, n_points, max_points)\n",
    "        self.pres = pres\n",
    "       \n",
    "        self.noise_scheduler = NoiseSchedulerDDPM(ddpm_params['beta_min'], ddpm_params['beta_max'], ddpm_params['n_steps'], ddpm_params['mode'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        data = super().__getitem__(idx)\n",
    "\n",
    "        # add noise\n",
    "        data, t, noise = self.noise_scheduler(data)\n",
    "\n",
    "        # sparse quantize\n",
    "        pts = data[:, :3].numpy()\n",
    "        coords = pts - np.min(pts, axis=0, keepdims=True)\n",
    "        coords, indices = sparse_quantize(coords, self.pres, return_index=True)\n",
    "\n",
    "        # coordinates as torch tensor\n",
    "        coords = torch.tensor(coords, dtype=torch.int)\n",
    "        # features (includes actual coordinates and colors)\n",
    "        feats = data[indices].float()\n",
    "        noise = noise[indices].float()\n",
    "        \n",
    "        noisy_pts = SparseTensor(coords=coords, feats=feats)\n",
    "        noise = SparseTensor(coords=coords, feats=noise)\n",
    "        \n",
    "        return {'input':noisy_pts, 't':t, 'noise':noise}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb84be56-e6eb-4635-a85a-ddb98b550b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "shapenetcolour = ShapeNetColorSparseNoisy('./data/ShapeNetColor/', category='Chair', max_points=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e4a6f3-6ac1-480f-90d2-f9f178644493",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "def get_sparse_datasets(path, category, pres=1e-5, n_points=2048, max_points=4096,\n",
    "                        ddpm_params = {'beta_min':0.0001, \n",
    "                                       'beta_max':0.02, \n",
    "                                       'n_steps':1000, \n",
    "                                       'mode':'linear' }):\n",
    "\n",
    "    tr_dataset = ShapeNetColorSparseNoisy(path, category, split='train', n_points=n_points, max_points=max_points, pres=pres, ddpm_params=ddpm_params) \n",
    "    te_dataset = ShapeNetColorSparseNoisy(path, category, split='test' , n_points=n_points, max_points=max_points, pres=pres, ddpm_params=ddpm_params)\n",
    "\n",
    "    return tr_dataset, te_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa2beca-bc10-4597-a175-e0d7b0be0419",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "def get_sparse_dataloaders(path, category, pres=1e-5, n_points=2048, max_points=4096, batch_size=32, num_workers=8,\n",
    "                           ddpm_params = {'beta_min':0.0001, \n",
    "                                          'beta_max':0.02, \n",
    "                                          'n_steps':1000, \n",
    "                                          'mode':'linear'}):\n",
    "    tr_dataset, te_dataset = get_sparse_datasets(path, category=category, pres=pres, n_points=n_points, max_points=max_points, ddpm_params=ddpm_params)\n",
    "\n",
    "    tr_dl = DataLoader(tr_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, drop_last=True, collate_fn=sparse_collate_fn)\n",
    "    te_dl = DataLoader(te_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, collate_fn=sparse_collate_fn)\n",
    "\n",
    "    return tr_dl, te_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99727b44-9ccf-4b73-a1f3-469d43c55a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_dl, te_dl = get_sparse_dataloaders('./data/ShapeNetColor/', 'Chair')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e01d61e-378d-4f15-8ff1-7bfd35b2a19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tr_dl), len(te_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c91bec-67e5-4ea3-af08-dac17c16afcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1ff322-6aa7-425f-be98-eb45a80d5d91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
